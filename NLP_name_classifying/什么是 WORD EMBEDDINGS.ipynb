{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORD EMBEDDING\n",
    "\n",
    "## 参考资料\n",
    "\n",
    "[什么是 word embedding?](https://www.zhihu.com/question/32275069/answer/61059440)\n",
    "\n",
    "[WORD EMBEDDINGS: ENCODING LEXICAL SEMANTICS](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#sphx-glr-beginner-nlp-word-embeddings-tutorial-py)\n",
    "\n",
    "[YJango的Word Embedding--介绍](https://zhuanlan.zhihu.com/p/27830489)\n",
    "\n",
    "## 概念解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码解析\n",
    "\n",
    "与我们制作独热向量（one-hot vectors）时候为每个单词定义一个唯一索引类似，在使用词嵌入的时候我们也需要为每个单词定义索引，方便查找。\n",
    "\n",
    "词嵌入被储存为了一个 $ |V|×D $ 的矩阵，其中 $ D $ 是嵌入的维度，所以某单词的索引为 $ i $，那么它就被嵌入在矩阵的第 $ i $ 行中。在代码中，从单词到索引的映射是一个名叫 word_to_ix 的字典。\n",
    "\n",
    "在 PyTorch 中，torch.nn.Embedding 可以完成词嵌入功能，包含两个参数：词表大小和嵌入维度。\n",
    "\n",
    "使用 torch.LongTensor 进行索引（这是一个 64-bit integer，带符号数）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa2600520d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Author: Robert Guthrie\n",
    "# 补充：Dr_David_S\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)  # 设定随机种子，返回一个torch.Generator对象"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们设置一个字典，用最简单的 hello world 做例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 0, 'world': 1}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "word_to_ix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 torch.nn.Embedding 方法准备进行词嵌入，参数的意义是：有2个词语，5个维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(2, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds = nn.Embedding(num_embeddings=2, embedding_dim=5)  # 2 words in vocab, 5 dimensional embeddings\n",
    "embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在将 hello 对应的值 0 转换为一个 longtensor 类型,两种方法都行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
    "lookup_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_tensor_test = torch.LongTensor([word_to_ix[\"hello\"]])\n",
    "lookup_tensor_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在使用之前定义的 embeds 模型对 hello 这个次进行词嵌入（或者说转换为词向量）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "hello_embed = embeds(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原来的代码到此就结束了，但是这里我们希望再试试对 world 的词嵌入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_tensor2 = torch.tensor([word_to_ix[\"world\"]], dtype=torch.long)\n",
    "lookup_tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1661, -1.5228,  0.3817, -1.0276, -0.5631]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "world_embed = embeds(lookup_tensor2)\n",
    "print(world_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上述结果来看，似乎对于 hello 的 embedding 没有涉及到 world？\n",
    "\n",
    "事实上，embeds只是一个随机生成的矩阵，目前其中的向量并不能代表任何意义，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519],\n",
       "        [-0.1661, -1.5228,  0.3817, -1.0276, -0.5631]], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">nn.Embedding.weight初始化分布符合标准正态分布 $ N(0,1) $，即均值 $\\mu=0$，方差 $\\sigma=1$ 的正态分布。\n",
    ">\n",
    ">因为这里的初始化参数只设定了两个单词，所以其中：\n",
    ">\n",
    ">某一行代表单词 'hello'\n",
    ">\n",
    ">另一行代表单词 'world'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一个 N-Gram 语言模型\n",
    "\n",
    "之前我们的 embeds 矩阵的初始值是随机生成的，现在我们将要计算一些训练样例的损失函数，然后使用反向传播去更新参数。\n",
    "\n",
    "### 文本处理\n",
    "\n",
    "首先设定参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2  # 指上下文的size\n",
    "EMBEDDING_DIM = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，上下文是指目标单词的前 $ n $ 个单词，或者后 $ n $ 个单词，或者前后都有总共 $ n $ 个单词。\n",
    "\n",
    "在这里，CONTEXT_SIZE 指的是上文的两个单词，意味着如果以后遇到上下文的情况，需要乘以二。\n",
    "\n",
    "EMBEDDING_DIM 指的是会将其 embedding 为十维向量。\n",
    "\n",
    "在本例中，我们使用的是莎士比亚的十四行诗，使用空格分词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">SONNET 2：\n",
    ">\n",
    ">当四十个冬天围攻你的朱颜，\n",
    ">\n",
    ">在你美的园地挖下深的战壕，\n",
    ">\n",
    ">你青春的华服，那么被人艳羡，\n",
    ">\n",
    ">将成褴褛的败絮，谁也不要瞧：\n",
    ">\n",
    ">那时人若问起你的美在何处，\n",
    ">\n",
    ">哪里是你那少壮年华的宝藏，\n",
    ">\n",
    ">你说，\"在我这双深陷的眼眶里，\n",
    ">\n",
    ">是贪婪的羞耻，和无益的颂扬。\"\n",
    ">\n",
    ">你的美的用途会更值得赞美，\n",
    ">\n",
    ">如果你能够说，\"我这宁馨小童\n",
    ">\n",
    ">将总结我的账，宽恕我的老迈，\"\n",
    ">\n",
    ">证实他的美在继承你的血统！\n",
    ">\n",
    ">这将使你在衰老的暮年更生，\n",
    ">\n",
    ">并使你垂冷的血液感到重温。\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When',\n",
       " 'forty',\n",
       " 'winters',\n",
       " 'shall',\n",
       " 'besiege',\n",
       " 'thy',\n",
       " 'brow,',\n",
       " 'And',\n",
       " 'dig',\n",
       " 'deep',\n",
       " 'trenches',\n",
       " 'in',\n",
       " 'thy',\n",
       " \"beauty's\",\n",
       " 'field,',\n",
       " 'Thy',\n",
       " \"youth's\",\n",
       " 'proud',\n",
       " 'livery',\n",
       " 'so',\n",
       " 'gazed',\n",
       " 'on',\n",
       " 'now,',\n",
       " 'Will',\n",
       " 'be',\n",
       " 'a',\n",
       " \"totter'd\",\n",
       " 'weed',\n",
       " 'of',\n",
       " 'small',\n",
       " 'worth',\n",
       " 'held:',\n",
       " 'Then',\n",
       " 'being',\n",
       " 'asked,',\n",
       " 'where',\n",
       " 'all',\n",
       " 'thy',\n",
       " 'beauty',\n",
       " 'lies,',\n",
       " 'Where',\n",
       " 'all',\n",
       " 'the',\n",
       " 'treasure',\n",
       " 'of',\n",
       " 'thy',\n",
       " 'lusty',\n",
       " 'days;',\n",
       " 'To',\n",
       " 'say,',\n",
       " 'within',\n",
       " 'thine',\n",
       " 'own',\n",
       " 'deep',\n",
       " 'sunken',\n",
       " 'eyes,',\n",
       " 'Were',\n",
       " 'an',\n",
       " 'all-eating',\n",
       " 'shame,',\n",
       " 'and',\n",
       " 'thriftless',\n",
       " 'praise.',\n",
       " 'How',\n",
       " 'much',\n",
       " 'more',\n",
       " 'praise',\n",
       " \"deserv'd\",\n",
       " 'thy',\n",
       " \"beauty's\",\n",
       " 'use,',\n",
       " 'If',\n",
       " 'thou',\n",
       " 'couldst',\n",
       " 'answer',\n",
       " \"'This\",\n",
       " 'fair',\n",
       " 'child',\n",
       " 'of',\n",
       " 'mine',\n",
       " 'Shall',\n",
       " 'sum',\n",
       " 'my',\n",
       " 'count,',\n",
       " 'and',\n",
       " 'make',\n",
       " 'my',\n",
       " 'old',\n",
       " \"excuse,'\",\n",
       " 'Proving',\n",
       " 'his',\n",
       " 'beauty',\n",
       " 'by',\n",
       " 'succession',\n",
       " 'thine!',\n",
       " 'This',\n",
       " 'were',\n",
       " 'to',\n",
       " 'be',\n",
       " 'new',\n",
       " 'made',\n",
       " 'when',\n",
       " 'thou',\n",
       " 'art',\n",
       " 'old,',\n",
       " 'And',\n",
       " 'see',\n",
       " 'thy',\n",
       " 'blood',\n",
       " 'warm',\n",
       " 'when',\n",
       " 'thou',\n",
       " \"feel'st\",\n",
       " 'it',\n",
       " 'cold.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按道理，我们在训练之前输入的数据应当是有标记的，我们暂时忽略这个事情（后面会说明）。\n",
    "\n",
    "然后我们要建立一个元组（tuple），元组的格式为：\n",
    "\n",
    "$$ ([word_{i-2}, word_{i-1}], word_{i}) $$\n",
    "\n",
    "其中 $word_{i}$ 就是目标词 $ target word $ \n",
    "\n",
    "根据以上方法遍历整首诗，然后生成多个元组，放入列表 trigrams 中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['When', 'forty'], 'winters'), (['forty', 'winters'], 'shall'), (['winters', 'shall'], 'besiege')]\n"
     ]
    }
   ],
   "source": [
    "# we should tokenize the input, but we will ignore that for now\n",
    "# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
    "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
    "            for i in range(len(test_sentence) - 2)]\n",
    "# print the first 3, just so you can see what they look like\n",
    "print(trigrams[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set() 函数创建一个无序不重复元素集，可以剔除掉重复的元素，同时可能可以在部分模型如 RNN 中打乱文档上下文，忽略掉 RNN 权重偏向后输入的词语的缺点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(test_sentence)  # 去重\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建网络\n",
    "\n",
    "接下来我们构建一个网络，名叫 NGramLanguageModeler："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)  # 这步实现了embedding\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上述网络来看，init 中存在三个层，分别是：\n",
    "\n",
    "- 词嵌入层（embedding层）\n",
    "- 第一个全连接层（linear1）\n",
    "- 第二个全连接层（linear2）\n",
    "\n",
    "算上前向传播的过程就是：\n",
    "\n",
    "- 词嵌入层\n",
    "- 转换为一维\n",
    "- 第一个全连接层\n",
    "- ReLU激活\n",
    "- 第二个全连接层\n",
    "- log_softmax输出\n",
    "\n",
    "### 设置损失函数和优化函数\n",
    "\n",
    "- 使用NLLLoss()，这个函数其实就是一个不带 log_softmax 的交叉熵损失函数。\n",
    "\n",
    "- 使用optim.SGD()，最传统的随机梯度下降，没什么好说的。\n",
    "\n",
    "需要注意的是，model 的输入参数有三个，第一个就是去重后的上下文三词元组数量，第二三个则是我们设定好的 CONTEXT_SIZE 和 EMBEDDING_DIM。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型\n",
    "\n",
    "注意：\n",
    "```Python\n",
    "for context, target in trigrams\n",
    "```\n",
    "这句话指的是我们的训练样本是上下文（其实只有上文），标签是当前词。这也是为什么之前没有标记的原因。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['When', 'forty'], 'winters'),\n",
       " (['forty', 'winters'], 'shall'),\n",
       " (['winters', 'shall'], 'besiege'),\n",
       " (['shall', 'besiege'], 'thy'),\n",
       " (['besiege', 'thy'], 'brow,'),\n",
       " (['thy', 'brow,'], 'And'),\n",
       " (['brow,', 'And'], 'dig'),\n",
       " (['And', 'dig'], 'deep'),\n",
       " (['dig', 'deep'], 'trenches'),\n",
       " (['deep', 'trenches'], 'in'),\n",
       " (['trenches', 'in'], 'thy'),\n",
       " (['in', 'thy'], \"beauty's\"),\n",
       " (['thy', \"beauty's\"], 'field,'),\n",
       " ([\"beauty's\", 'field,'], 'Thy'),\n",
       " (['field,', 'Thy'], \"youth's\"),\n",
       " (['Thy', \"youth's\"], 'proud'),\n",
       " ([\"youth's\", 'proud'], 'livery'),\n",
       " (['proud', 'livery'], 'so'),\n",
       " (['livery', 'so'], 'gazed'),\n",
       " (['so', 'gazed'], 'on'),\n",
       " (['gazed', 'on'], 'now,'),\n",
       " (['on', 'now,'], 'Will'),\n",
       " (['now,', 'Will'], 'be'),\n",
       " (['Will', 'be'], 'a'),\n",
       " (['be', 'a'], \"totter'd\"),\n",
       " (['a', \"totter'd\"], 'weed'),\n",
       " ([\"totter'd\", 'weed'], 'of'),\n",
       " (['weed', 'of'], 'small'),\n",
       " (['of', 'small'], 'worth'),\n",
       " (['small', 'worth'], 'held:'),\n",
       " (['worth', 'held:'], 'Then'),\n",
       " (['held:', 'Then'], 'being'),\n",
       " (['Then', 'being'], 'asked,'),\n",
       " (['being', 'asked,'], 'where'),\n",
       " (['asked,', 'where'], 'all'),\n",
       " (['where', 'all'], 'thy'),\n",
       " (['all', 'thy'], 'beauty'),\n",
       " (['thy', 'beauty'], 'lies,'),\n",
       " (['beauty', 'lies,'], 'Where'),\n",
       " (['lies,', 'Where'], 'all'),\n",
       " (['Where', 'all'], 'the'),\n",
       " (['all', 'the'], 'treasure'),\n",
       " (['the', 'treasure'], 'of'),\n",
       " (['treasure', 'of'], 'thy'),\n",
       " (['of', 'thy'], 'lusty'),\n",
       " (['thy', 'lusty'], 'days;'),\n",
       " (['lusty', 'days;'], 'To'),\n",
       " (['days;', 'To'], 'say,'),\n",
       " (['To', 'say,'], 'within'),\n",
       " (['say,', 'within'], 'thine'),\n",
       " (['within', 'thine'], 'own'),\n",
       " (['thine', 'own'], 'deep'),\n",
       " (['own', 'deep'], 'sunken'),\n",
       " (['deep', 'sunken'], 'eyes,'),\n",
       " (['sunken', 'eyes,'], 'Were'),\n",
       " (['eyes,', 'Were'], 'an'),\n",
       " (['Were', 'an'], 'all-eating'),\n",
       " (['an', 'all-eating'], 'shame,'),\n",
       " (['all-eating', 'shame,'], 'and'),\n",
       " (['shame,', 'and'], 'thriftless'),\n",
       " (['and', 'thriftless'], 'praise.'),\n",
       " (['thriftless', 'praise.'], 'How'),\n",
       " (['praise.', 'How'], 'much'),\n",
       " (['How', 'much'], 'more'),\n",
       " (['much', 'more'], 'praise'),\n",
       " (['more', 'praise'], \"deserv'd\"),\n",
       " (['praise', \"deserv'd\"], 'thy'),\n",
       " ([\"deserv'd\", 'thy'], \"beauty's\"),\n",
       " (['thy', \"beauty's\"], 'use,'),\n",
       " ([\"beauty's\", 'use,'], 'If'),\n",
       " (['use,', 'If'], 'thou'),\n",
       " (['If', 'thou'], 'couldst'),\n",
       " (['thou', 'couldst'], 'answer'),\n",
       " (['couldst', 'answer'], \"'This\"),\n",
       " (['answer', \"'This\"], 'fair'),\n",
       " ([\"'This\", 'fair'], 'child'),\n",
       " (['fair', 'child'], 'of'),\n",
       " (['child', 'of'], 'mine'),\n",
       " (['of', 'mine'], 'Shall'),\n",
       " (['mine', 'Shall'], 'sum'),\n",
       " (['Shall', 'sum'], 'my'),\n",
       " (['sum', 'my'], 'count,'),\n",
       " (['my', 'count,'], 'and'),\n",
       " (['count,', 'and'], 'make'),\n",
       " (['and', 'make'], 'my'),\n",
       " (['make', 'my'], 'old'),\n",
       " (['my', 'old'], \"excuse,'\"),\n",
       " (['old', \"excuse,'\"], 'Proving'),\n",
       " ([\"excuse,'\", 'Proving'], 'his'),\n",
       " (['Proving', 'his'], 'beauty'),\n",
       " (['his', 'beauty'], 'by'),\n",
       " (['beauty', 'by'], 'succession'),\n",
       " (['by', 'succession'], 'thine!'),\n",
       " (['succession', 'thine!'], 'This'),\n",
       " (['thine!', 'This'], 'were'),\n",
       " (['This', 'were'], 'to'),\n",
       " (['were', 'to'], 'be'),\n",
       " (['to', 'be'], 'new'),\n",
       " (['be', 'new'], 'made'),\n",
       " (['new', 'made'], 'when'),\n",
       " (['made', 'when'], 'thou'),\n",
       " (['when', 'thou'], 'art'),\n",
       " (['thou', 'art'], 'old,'),\n",
       " (['art', 'old,'], 'And'),\n",
       " (['old,', 'And'], 'see'),\n",
       " (['And', 'see'], 'thy'),\n",
       " (['see', 'thy'], 'blood'),\n",
       " (['thy', 'blood'], 'warm'),\n",
       " (['blood', 'warm'], 'when'),\n",
       " (['warm', 'when'], 'thou'),\n",
       " (['when', 'thou'], \"feel'st\"),\n",
       " (['thou', \"feel'st\"], 'it'),\n",
       " ([\"feel'st\", 'it'], 'cold.')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[521.878231048584, 519.3365745544434, 516.814404964447, 514.3094692230225, 511.820321559906, 509.3474576473236, 506.8883717060089, 504.44230675697327, 502.0088152885437, 499.5859134197235, 497.1743552684784, 494.77218222618103, 492.3794560432434, 489.99517035484314, 487.6185758113861, 485.2484574317932, 482.88393354415894, 480.52435755729675, 478.17004323005676, 475.8179910182953, 473.46933341026306, 471.12329840660095, 468.77851271629333, 466.4353778362274, 464.09136986732483, 461.7471446990967, 459.4029574394226, 457.0576317310333, 454.7120842933655, 452.36570167541504, 450.0161933898926, 447.6633388996124, 445.30811834335327, 442.9496729373932, 440.58748269081116, 438.2198576927185, 435.84934091567993, 433.47243475914, 431.08960461616516, 428.7008910179138, 426.3039280176163, 423.9011753797531, 421.4890441894531, 419.06925213336945, 416.6411648988724, 414.2071763277054, 411.7645524740219, 409.31272637844086, 406.85250759124756, 404.38154780864716, 401.9022305011749, 399.41368556022644, 396.9146156311035, 394.40673315525055, 391.88696575164795, 389.3584473133087, 386.81902408599854, 384.26745438575745, 381.7064702510834, 379.13651263713837, 376.55655789375305, 373.9669705629349, 371.3660273551941, 368.7571129798889, 366.1380726099014, 363.51042807102203, 360.87202060222626, 358.2276539206505, 355.57122790813446, 352.90467125177383, 350.2294805049896, 347.543748319149, 344.8500393629074, 342.14760106801987, 339.43483406305313, 336.7145869731903, 333.9873094558716, 331.2541506290436, 328.50989919900894, 325.7629744410515, 323.00879752635956, 320.2472200989723, 317.4782840013504, 314.70350271463394, 311.92436611652374, 309.13747519254684, 306.3443467617035, 303.54760533571243, 300.74621069431305, 297.9421377778053, 295.1365837454796, 292.3261062502861, 289.5162915587425, 286.7029417157173, 283.89026767015457, 281.07749515771866, 278.2642457485199, 275.4527310729027, 272.640529692173, 269.83250826597214]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    for context, target in trigrams:\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        # 这一步把上下文单词转换为字典中对应单词的值，作为tensor\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们得到了一个损失逐渐降低的模型，可以看出这个模型的结果是收敛的，如果继续训练下去，可能会更准确，但是到此为止吧。\n",
    "\n",
    "## 练习：计算词嵌入：CBOW模型\n",
    "\n",
    "连续词袋模型（Continuous Bag-of-Words model ），又被称为CBOW模型，经常用于自然语言处理深度学习。\n",
    "\n",
    "CBOW模型使用一段文本的中间词作为目标词，即给出目标词的上下文单词作为训练数据。CBOW去除了上下文各词语的词序信息，使用的是上下文各词的词向量的平均值。\n",
    "\n",
    "CBOW对于一段长度为 $ n $ 的训练样本：$ w_{i-(n-1)}, ...,w_i $ ，其输入为: $$ x = \\frac {1}{n - 1}\\sum_{w_j \\in c}e(w_j) $$\n",
    "\n",
    "其中 $ w $ 是目标词，即 $ w_i $， $ c $ 是上下文，上式就是将输入求平均。\n",
    "\n",
    "故CBOW模型根据上下文的表示，直接对目标词进行预测： $$ P(w|c) = \\frac{\\exp(e'(w)^Tx)}{\\sum_{w'\\in V}\\exp \\lgroup e'(w')^Tx\\rgroup} $$ \n",
    "\n",
    "CBOW的目标是最大化 $$ \\sum_{(w,c)\\in D}\\log P(w|c) $$\n",
    "\n",
    "其中 $D$ 指的是整段语料（或者说整篇文章）。\n",
    "\n",
    "要在PyTorch中实现这个模型，需要注意的是：\n",
    "\n",
    "- 考虑一下需要定义哪些参数\n",
    "- 确保你清楚每一步需要哪些形状的矩阵，使用 .view() 函数来 reshape 矩阵\n",
    "\n",
    "### 1.分割语料\n",
    "\n",
    "代码示例如下，第一步，先按空格分隔语料中的单词，其中 2 表示上文两个单词加下文两个单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 set() 函数将分词唯一化并打乱顺序，然后建立一个名叫 word_to_ix 的字典，字典的 key 是单词，value 是单词的索引。\n",
    "\n",
    "接下来使用data列表来储存文本和目标了，把第 $i$ 个单词作为目标，把第$[i-2, i-1, i+1, i+2]$ 个单词作为上下文（特征），注意这里使用的是有序的语料。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['We', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['about', 'to', 'the', 'idea'], 'study'), (['to', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'a'], 'idea')]\n",
      "58\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "# By deriving a set from `raw_text`, we deduplicate the array\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    # context保存目标单词上下文，分别2个单词\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    # target保存目标单词\n",
    "    target = raw_text[i]\n",
    "    # 存入data列表中\n",
    "    data.append((context, target))\n",
    "    \n",
    "print(data[:5])\n",
    "print(len(data))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出我们的上下文目标词组有58个，但是非重复单词有49个。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.建立CBOW类\n",
    "\n",
    "建立 CBOW 类，由于本节有习题性质，CBOW 类并没有内容，需要自己填空。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([33, 17, 36, 37])\n"
     ]
    }
   ],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        pass\n",
    "\n",
    "# create your model and train.  here are some functions to help you make\n",
    "# the data ready for use by your module\n",
    "\n",
    "\n",
    "def make_context_vector(context, word_to_ix):\n",
    "    \"\"\"从字典中抽取word对应的value\n",
    "    \n",
    "    输入：\n",
    "    context:上下文列表（四个单词）\n",
    "    word_to_ix:单词的字典\n",
    "    \n",
    "    输出：context中的单词对应的longtensor\n",
    "    \"\"\"\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "first_context = make_context_vector(data[0][0], word_to_ix)  # example\n",
    "print(first_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里给了一个例子，就是\n",
    "\n",
    "```Python\n",
    "make_context_vector(data[0][0], word_to_ix)  # example\n",
    "```\n",
    "简单分析一下：\n",
    "\n",
    "```Python\n",
    "data[0][0]\n",
    "```\n",
    "指的就是前面 data 列表中第一个tuple中的第一个元素，即:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'are', 'to', 'study']\n"
     ]
    }
   ],
   "source": [
    "print(data[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述四个单词在字典 word_to_ix 中对应的值如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "17\n",
      "36\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "print(word_to_ix['We'])\n",
    "print(word_to_ix['are'])\n",
    "print(word_to_ix['to'])\n",
    "print(word_to_ix['study'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "故其组成的 longtensor 自然就是 tensor([20, 27, 26, 21]) 啦~\n",
    "\n",
    "### 简单embedding演示\n",
    "\n",
    "现在，使用 torch.nn.Embedding 方法建立一个embedding模型，目标是去重后的49个词，每个词20维。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(49, 20)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds = nn.Embedding(num_embeddings=49, embedding_dim=20)  # 58 words in vocab, 20 dimensional embeddings\n",
    "embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们尝试使用上面的 embeds 模型将 first_context 向量化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2718, -1.6699,  2.0873, -0.5097, -0.4212, -0.9803,  0.7608, -0.8248,\n",
      "          1.0795, -0.7436, -1.1971, -0.8597,  0.6141, -0.0859, -0.2654,  1.8039,\n",
      "         -0.0279,  1.2599, -0.5165,  0.3539],\n",
      "        [ 1.8870, -0.3962,  1.3817, -1.1175,  0.1472, -1.4784, -0.4961,  0.8160,\n",
      "         -1.4932, -1.0810, -0.8416,  1.2265, -0.4613, -1.1796, -2.0502, -1.4492,\n",
      "         -0.3409, -2.8001, -0.3261,  1.0929],\n",
      "        [-1.0197, -0.2241,  1.2898, -0.6360, -1.6775, -0.3627, -0.1158, -0.7729,\n",
      "          0.6541, -1.0522,  0.6729,  2.1099,  0.1009,  0.8126,  0.8929,  0.2729,\n",
      "         -0.0568, -0.7972, -0.3836,  2.1871],\n",
      "        [ 1.3518,  0.6369, -1.5268,  0.4858, -0.6772,  1.0290,  1.2745,  0.6527,\n",
      "         -0.3394,  0.4518, -2.0870,  0.8774, -1.5316,  1.6614, -0.9597, -1.0855,\n",
      "          1.3273, -0.6726,  0.6029, -0.5115]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "first_context_embed = embeds(first_context)\n",
    "print(first_context_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出已经成功的完成了我们向量化的第一步实验，接下来的任务就是把所有的上下文和目标单词向量化，然后建立一个 CBOW 模型对这些向量进行训练。\n",
    "\n",
    "顺便看看 embeds 的初始化，应当是随机初始化了一个符合标准正态分布 $ N(0,1) $的 49 行，20 列的矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-6.4612e-01,  3.2699e-01, -1.2190e+00, -5.4798e-01, -1.7273e+00,\n",
       "         -7.3260e-01,  6.2941e-01, -2.3110e-01,  4.3045e-01,  1.7503e+00,\n",
       "         -2.9139e-01, -4.2367e-01,  5.4413e-01,  1.6597e+00, -5.6447e-01,\n",
       "         -7.9011e-01,  4.2483e-01, -2.5986e+00, -9.2568e-01, -8.6511e-01],\n",
       "        [-1.6726e-01,  1.5749e+00, -1.1857e+00,  1.2867e+00, -5.1799e-01,\n",
       "          2.3175e+00, -1.9279e+00,  1.2128e+00,  7.7894e-01,  3.8469e-02,\n",
       "         -1.1871e+00,  3.4312e-01,  6.9343e-01,  1.0216e+00, -7.4495e-01,\n",
       "          4.6306e-02, -1.5447e+00, -1.5220e+00,  9.3887e-01, -5.8114e-01],\n",
       "        [ 1.9286e+00, -1.0957e+00,  6.8782e-01, -5.4693e-01, -5.5047e-01,\n",
       "          5.0876e-01,  8.9655e-01,  4.8738e-01, -2.6993e-01,  3.3697e-01,\n",
       "          3.7492e-01, -3.6387e-01, -5.9859e-02,  8.9039e-01,  1.6794e-01,\n",
       "         -1.8218e+00, -2.9632e-01,  2.2457e-01,  6.6170e-01,  1.2258e+00],\n",
       "        [ 1.5298e-01,  3.1138e-01,  4.5681e-01,  6.1812e-01,  5.5626e-01,\n",
       "          8.4204e-02, -1.0673e+00,  5.8148e-01,  1.0307e+00, -1.1797e+00,\n",
       "         -8.3080e-01, -8.9505e-01, -1.6252e+00, -1.8404e-01, -2.0350e-01,\n",
       "         -1.5157e-01,  2.2578e-01,  1.2555e-01,  7.3928e-01, -6.3752e-01],\n",
       "        [-2.9521e-01, -5.0068e-02,  1.2459e+00, -1.1685e+00,  8.6762e-01,\n",
       "         -1.0231e+00, -2.5141e-01,  6.6077e-01, -1.3057e+00, -3.3575e-01,\n",
       "          2.0215e+00,  9.0360e-01, -8.5247e-01, -1.4626e+00, -6.7340e-01,\n",
       "          1.1555e+00,  4.6490e-02, -6.1870e-01, -6.0584e-01,  6.2081e-01],\n",
       "        [ 6.7061e-01, -4.1660e-01,  5.0074e-01, -4.2875e-01, -9.3269e-01,\n",
       "         -5.2552e-01,  7.1523e-01, -1.1578e+00, -1.8319e+00, -2.0151e+00,\n",
       "         -1.0206e-01, -1.7675e-01, -1.1185e+00, -1.3922e+00, -3.9210e-01,\n",
       "          9.9712e-02,  5.1237e-02,  2.6712e-01,  4.2794e-01, -1.3539e+00],\n",
       "        [-3.9830e-01,  4.1003e-01,  1.4477e+00,  6.8512e-01,  1.0783e+00,\n",
       "         -7.3562e-01,  1.4571e+00,  3.7548e-01,  2.9159e-01,  3.0918e-02,\n",
       "          2.1513e-01,  1.1820e+00, -1.6783e-01, -6.0394e-01,  1.2088e-01,\n",
       "          1.8014e+00, -2.8230e-01, -9.4255e-01,  8.6056e-01, -2.8156e-01],\n",
       "        [ 5.8663e-01, -6.3155e-01, -4.4007e-01, -2.6109e+00, -4.2228e-01,\n",
       "         -5.3477e-01, -8.0544e-01,  3.6803e-02, -1.0620e+00, -1.1157e+00,\n",
       "          9.4445e-01,  8.8128e-01, -1.7200e+00, -1.0377e+00,  2.7694e-01,\n",
       "          5.4669e-02,  7.2963e-01,  1.0075e+00,  1.9498e-01, -2.4707e-01],\n",
       "        [ 1.4736e-01,  1.5644e+00, -8.4295e-01, -8.4363e-01,  2.8921e-01,\n",
       "         -2.0141e+00,  1.2655e+00, -8.5526e-01, -7.6453e-02,  2.2450e-01,\n",
       "          1.7620e-01,  6.5641e-01, -9.8634e-01, -1.2625e-01,  4.1285e-01,\n",
       "          5.4039e-01, -5.1595e-01, -7.3100e-01,  3.3077e-01, -2.2762e-01],\n",
       "        [-1.2758e+00, -7.3872e-01, -6.8438e-03, -3.4149e-01,  2.1878e-01,\n",
       "         -5.9360e-01,  7.0917e-01,  2.7720e-01,  3.4164e-02, -9.9656e-01,\n",
       "         -5.8303e-01,  1.4202e+00, -1.4229e+00, -1.5302e-01, -5.4354e-01,\n",
       "         -3.2468e-01, -1.1976e+00,  2.1555e-01,  4.0381e-01,  9.1457e-01],\n",
       "        [-5.2225e-01, -8.6608e-01,  4.3713e-01, -2.5476e+00, -1.7613e+00,\n",
       "         -2.0757e+00, -9.8712e-01, -8.9588e-02,  6.1980e-01, -1.3842e-01,\n",
       "          1.3611e+00,  1.0847e+00,  1.4003e+00,  5.0501e-01, -4.9259e-01,\n",
       "          1.6010e+00,  4.5631e-01,  6.9477e-01,  6.9024e-01,  2.3155e-01],\n",
       "        [-3.4351e-01, -8.4703e-01, -3.7904e-02, -5.4371e-01,  2.1110e-01,\n",
       "          5.7432e-03, -1.0064e+00, -3.5164e-01, -6.7232e-01,  8.5239e-01,\n",
       "          2.2842e-02, -1.2048e-01,  3.2285e-01, -6.6877e-01,  1.9705e-01,\n",
       "          6.5321e-01,  5.4131e-01, -1.9713e-01, -7.7884e-01, -2.9739e-01],\n",
       "        [ 9.9538e-01,  2.0110e+00, -3.0361e+00, -8.8617e-01, -1.0888e+00,\n",
       "         -1.8985e+00, -5.3017e-01,  1.3378e+00,  3.3699e-01,  4.5674e-01,\n",
       "         -5.9711e-01,  1.5383e-01, -2.9491e-03, -4.4786e-01, -6.7690e-01,\n",
       "          2.4452e+00, -9.2435e-01, -3.6381e-01, -3.1103e-01, -2.1614e-01],\n",
       "        [-8.5986e-01,  1.3890e+00,  1.0275e+00,  2.3629e+00,  4.9995e-01,\n",
       "         -8.1681e-01, -3.5762e-01,  1.0356e+00,  1.1070e+00, -2.1964e+00,\n",
       "          2.9737e-01,  7.0117e-01, -2.0057e-01, -2.4837e-01, -1.7697e+00,\n",
       "          1.3014e+00, -2.6152e-01,  1.2984e+00,  1.0495e-01, -6.3099e-01],\n",
       "        [ 1.8581e+00, -2.2235e-01, -2.8063e-01, -8.2349e-02, -1.9687e+00,\n",
       "          8.0719e-01,  6.3672e-01,  2.1958e+00, -1.3211e+00,  3.0555e-01,\n",
       "         -3.5142e-01, -1.2690e-01,  5.0159e-01,  1.9683e-01,  1.1696e+00,\n",
       "          5.8625e-01,  1.2153e+00, -1.3427e+00,  5.3224e-02, -1.5316e+00],\n",
       "        [-8.5482e-01, -3.0829e-01, -1.7640e+00, -1.4122e+00,  8.7147e-01,\n",
       "         -7.8012e-01, -1.8653e+00,  2.2217e-01, -1.3652e+00, -8.1494e-03,\n",
       "         -5.5762e-01, -1.2324e-01, -9.8862e-01,  1.0437e+00, -1.8879e+00,\n",
       "         -1.0104e+00,  5.2885e-01, -9.3542e-01, -2.9633e-01, -1.0109e+00],\n",
       "        [-1.2749e+00,  1.5717e+00, -6.5870e-01, -7.1776e-01, -4.9416e-01,\n",
       "         -3.8299e-01,  9.7377e-01,  3.0006e-01,  1.8134e+00, -2.1868e-01,\n",
       "          9.7228e-01,  1.3626e+00,  1.1661e+00,  9.6096e-01,  9.3226e-01,\n",
       "          6.4027e-02, -9.6594e-01,  5.4543e-01, -4.4378e-01,  1.3148e+00],\n",
       "        [ 1.8870e+00, -3.9624e-01,  1.3817e+00, -1.1175e+00,  1.4716e-01,\n",
       "         -1.4784e+00, -4.9605e-01,  8.1603e-01, -1.4932e+00, -1.0810e+00,\n",
       "         -8.4163e-01,  1.2265e+00, -4.6127e-01, -1.1796e+00, -2.0502e+00,\n",
       "         -1.4492e+00, -3.4088e-01, -2.8001e+00, -3.2608e-01,  1.0929e+00],\n",
       "        [-3.9485e-01, -1.1922e+00, -2.7189e+00, -3.5492e-01, -7.6252e-02,\n",
       "         -5.6123e-01,  7.9164e-01,  7.0351e-01,  7.8184e-02, -7.8675e-01,\n",
       "         -5.1341e-01,  1.8308e-01, -1.2101e+00, -3.8712e-01, -3.3068e-01,\n",
       "         -2.0892e+00, -1.9865e+00, -9.5524e-01,  1.4961e-01, -1.1350e+00],\n",
       "        [ 8.2768e-01,  1.6810e+00,  2.0368e+00, -2.4623e-03,  5.8344e-01,\n",
       "          9.5311e-02, -1.4669e+00, -7.8617e-01, -1.2182e+00,  6.7451e-01,\n",
       "         -6.5318e-01, -1.6200e+00, -2.6747e-01, -6.1861e-01,  2.4425e+00,\n",
       "          1.1740e+00,  1.5380e+00, -8.7476e-01, -9.1974e-01, -6.0645e-01],\n",
       "        [-1.4582e+00,  1.2675e-01,  2.0489e-01, -2.3162e-01,  3.2185e-01,\n",
       "          3.0425e-01, -4.6609e-01,  1.2483e+00, -2.3342e-01,  3.2490e-02,\n",
       "          6.0111e-01, -8.4288e-01, -2.9537e-01, -5.8946e-01,  4.2486e-01,\n",
       "         -1.5064e-02, -9.1362e-01,  1.4877e-01, -9.1343e-01, -4.9254e-01],\n",
       "        [-8.5386e-01,  1.4438e-01,  1.5790e-02,  1.1071e+00, -5.5792e-01,\n",
       "         -4.6533e-01,  8.5663e-03, -6.1182e-01,  1.8152e+00,  1.9323e+00,\n",
       "          9.4969e-01, -8.7778e-01, -2.4529e-02,  1.5871e+00,  9.7879e-02,\n",
       "          1.2805e-01,  2.4881e+00,  6.0889e-01, -3.2600e-01, -6.1483e-01],\n",
       "        [-7.5268e-01,  1.0995e+00,  4.6630e-01, -1.0484e+00, -5.0767e-01,\n",
       "          2.1551e-01, -7.1084e-01, -4.7125e-02, -4.9392e-01, -5.4741e-01,\n",
       "          1.8425e+00, -8.7697e-01,  1.1133e+00, -1.2210e+00,  5.8803e-01,\n",
       "          5.6924e-01, -7.8297e-01,  5.1035e-01, -2.0215e+00,  4.4403e-01],\n",
       "        [ 2.1701e+00, -1.1956e+00, -4.9094e-01,  9.2730e-02, -9.4234e-02,\n",
       "          6.4589e-02,  8.0516e-01,  1.1450e+00,  2.5630e+00,  5.4765e-02,\n",
       "         -1.0666e+00, -9.1730e-02, -3.6984e-01,  6.5440e-01, -1.6191e+00,\n",
       "          5.7489e-01,  6.4147e-02, -4.6030e-01,  2.0330e-01, -1.0517e+00],\n",
       "        [-1.7701e+00,  1.6594e+00, -1.0174e-01,  8.1066e-02, -7.8780e-01,\n",
       "         -6.0248e-02,  1.2231e+00,  2.1588e+00, -2.8501e-01,  1.1827e-01,\n",
       "         -2.2164e-01, -2.6451e-01, -8.6130e-02,  2.1302e+00,  2.0088e+00,\n",
       "         -9.9897e-01,  6.4803e-02,  9.9206e-01,  1.5756e+00,  1.1190e-02],\n",
       "        [ 1.6576e+00, -2.5640e-01, -1.0941e+00,  3.5309e-01,  9.7882e-01,\n",
       "         -2.3927e+00,  6.5126e-01,  1.3049e+00,  1.4782e+00, -1.6622e-01,\n",
       "          9.3779e-01, -5.7441e-01,  1.1132e+00,  1.9803e+00,  4.2195e-01,\n",
       "          1.7182e-01, -1.4453e+00,  7.3965e-01,  7.9245e-01,  1.6938e+00],\n",
       "        [-1.3664e-01, -2.2897e-01, -5.7908e-01, -9.7284e-01, -1.0591e+00,\n",
       "         -2.1632e+00, -8.5437e-02,  1.1922e+00, -2.3391e+00, -1.6168e-01,\n",
       "         -3.6422e-01,  4.0754e-01,  5.6819e-01,  1.3333e+00, -1.1093e+00,\n",
       "         -1.0586e+00,  8.2037e-01, -1.2043e+00,  4.8871e-01,  1.6424e-01],\n",
       "        [ 1.7604e+00, -9.5568e-01, -7.1784e-01,  1.0461e+00,  3.4570e-01,\n",
       "          7.7037e-01,  1.3696e+00,  1.0393e+00, -9.4159e-02,  7.0077e-01,\n",
       "          2.8140e+00, -2.0784e-02,  4.7127e-01,  8.5660e-01, -1.0976e-01,\n",
       "         -1.7177e+00,  9.6252e-01, -8.5019e-01,  3.6280e-02,  8.5906e-01],\n",
       "        [ 1.6128e-01,  3.0219e-01, -1.4268e-01, -1.6845e+00,  1.6044e-01,\n",
       "         -6.7871e-01,  5.5636e-01, -2.6304e+00, -7.0924e-01,  5.2949e-01,\n",
       "          3.7346e-01, -1.8187e-01, -4.8180e-01,  3.0808e-01,  2.6557e-01,\n",
       "          1.1537e+00,  5.1182e-01,  9.2150e-01, -7.6625e-01, -1.1498e+00],\n",
       "        [-1.1761e+00,  4.0563e-01, -6.7374e-01, -1.5877e-01, -1.3063e+00,\n",
       "          6.6504e-01, -3.7262e-01, -1.3363e+00, -4.0013e-01, -1.8567e-01,\n",
       "          1.4302e+00,  2.5542e-01,  1.6355e-02,  1.0380e+00, -1.4176e+00,\n",
       "          2.5315e-01, -1.2685e+00,  9.3684e-01, -2.7396e+00,  6.3636e-01],\n",
       "        [ 1.2978e+00, -5.3488e-01, -9.8525e-01, -4.7698e-01, -2.0146e+00,\n",
       "         -1.6188e+00, -1.3016e+00,  6.0365e-01,  9.8307e-01,  9.3767e-02,\n",
       "          9.5827e-01,  3.1881e-01, -1.3424e+00, -4.3233e-03,  1.4535e-01,\n",
       "          7.1859e-01, -6.1634e-01,  9.9340e-01,  8.7201e-01,  2.0633e-01],\n",
       "        [ 1.3931e-01,  5.5169e-01,  1.0486e+00, -3.6740e-01, -1.0867e-01,\n",
       "         -2.6993e-01, -2.1083e+00,  1.0732e-01,  4.5863e-01,  5.2128e-01,\n",
       "          2.1124e+00, -2.1249e-01,  4.7540e-02, -4.9242e-01, -1.2577e-01,\n",
       "          8.7444e-01, -8.7877e-01, -5.5027e-01,  9.1300e-01,  5.5225e-01],\n",
       "        [-1.3960e-01, -1.2914e-01,  1.5105e+00,  3.3066e+00,  5.0361e-01,\n",
       "         -6.7854e-01, -8.5246e-01,  6.8996e-01,  6.4889e-01,  9.7900e-01,\n",
       "         -1.5172e+00, -1.5666e+00,  3.4575e-01, -6.0505e-01,  4.6073e-01,\n",
       "          2.6892e+00, -2.6866e-01,  5.7213e-01,  6.7731e-01, -1.2817e+00],\n",
       "        [-2.7178e-01, -1.6699e+00,  2.0873e+00, -5.0972e-01, -4.2117e-01,\n",
       "         -9.8028e-01,  7.6082e-01, -8.2479e-01,  1.0795e+00, -7.4356e-01,\n",
       "         -1.1971e+00, -8.5965e-01,  6.1411e-01, -8.5886e-02, -2.6542e-01,\n",
       "          1.8039e+00, -2.7877e-02,  1.2599e+00, -5.1647e-01,  3.5394e-01],\n",
       "        [-4.7843e-01, -6.0368e-01,  2.8307e+00,  6.2515e-01, -2.4328e-01,\n",
       "          1.6181e-01, -5.7773e-01, -1.2711e+00,  7.1220e-01,  2.6310e-01,\n",
       "          1.2436e+00, -3.8175e-01,  8.8800e-02, -6.9172e-01,  1.2000e+00,\n",
       "         -1.0698e+00,  2.3835e+00,  3.0750e+00, -1.9787e-01, -6.4660e-01],\n",
       "        [ 1.5260e-02, -9.7733e-01, -1.0556e+00, -1.6385e-02,  1.0676e+00,\n",
       "          9.4159e-01, -1.2145e-01,  3.1755e-01,  3.0020e-02, -1.1559e+00,\n",
       "         -1.6015e+00,  8.2630e-01, -2.6137e-01, -9.1525e-02,  1.2870e+00,\n",
       "          8.2858e-01, -1.2999e+00, -3.9609e-01, -5.6304e-02,  4.8650e-01],\n",
       "        [-1.0197e+00, -2.2411e-01,  1.2898e+00, -6.3598e-01, -1.6775e+00,\n",
       "         -3.6266e-01, -1.1584e-01, -7.7289e-01,  6.5409e-01, -1.0522e+00,\n",
       "          6.7291e-01,  2.1099e+00,  1.0090e-01,  8.1256e-01,  8.9294e-01,\n",
       "          2.7290e-01, -5.6792e-02, -7.9719e-01, -3.8362e-01,  2.1871e+00],\n",
       "        [ 1.3518e+00,  6.3693e-01, -1.5268e+00,  4.8577e-01, -6.7718e-01,\n",
       "          1.0290e+00,  1.2745e+00,  6.5274e-01, -3.3937e-01,  4.5182e-01,\n",
       "         -2.0870e+00,  8.7737e-01, -1.5316e+00,  1.6614e+00, -9.5973e-01,\n",
       "         -1.0855e+00,  1.3273e+00, -6.7264e-01,  6.0293e-01, -5.1152e-01],\n",
       "        [-1.0879e-01,  6.6156e-01,  8.1757e-01, -5.9691e-02, -2.2911e+00,\n",
       "          5.2300e-01, -4.7091e-01, -1.0084e+00,  2.9349e+00, -2.1103e-01,\n",
       "          1.4044e+00,  2.1109e-01,  3.3494e-01, -6.5218e-01,  4.0770e-01,\n",
       "         -1.0275e+00,  8.2848e-01,  3.7940e-01,  3.9498e-01,  1.9932e-02],\n",
       "        [ 2.2044e-01, -1.0154e+00,  4.3733e-01,  4.9727e-01, -1.4694e+00,\n",
       "          1.7094e+00,  1.9512e+00,  1.6500e+00, -2.9680e-01,  6.0425e-01,\n",
       "         -5.9361e-01,  9.6215e-01,  2.9756e-01, -8.9682e-01, -1.4268e+00,\n",
       "          6.6802e-01,  1.4990e+00,  3.0449e-01,  5.2850e-01,  3.4978e-01],\n",
       "        [-6.5679e-01, -5.5601e-01, -1.3267e+00,  1.3600e+00, -8.8738e-01,\n",
       "          9.3081e-01, -1.8860e+00, -1.2435e+00, -8.2652e-01, -7.5018e-01,\n",
       "          1.0114e-03, -1.7571e+00,  5.4194e-01, -1.1561e-01, -5.4410e-01,\n",
       "          9.3850e-01, -5.7520e-01, -4.3078e-01, -2.4131e-01, -9.2509e-01],\n",
       "        [-1.8558e-01, -6.9211e-01,  1.7669e+00, -7.7688e-01, -4.9083e-01,\n",
       "         -1.6007e+00, -6.0462e-01,  4.7312e-01,  3.1911e-01, -2.3392e-01,\n",
       "         -4.3389e-01, -8.9495e-01,  3.1844e-01,  1.7744e+00, -3.6426e-02,\n",
       "         -6.3042e-01, -1.8489e-01, -2.6009e+00, -5.7242e-01, -1.5001e-01],\n",
       "        [-1.0017e+00, -2.7486e-01, -5.4458e-01, -3.7835e-02,  7.4503e-01,\n",
       "         -8.3260e-01, -1.0409e+00,  5.0861e-01,  6.5756e-01,  1.7431e-01,\n",
       "          1.0688e+00,  5.1466e-01,  7.8011e-01,  5.0827e-01,  9.4445e-01,\n",
       "         -7.0368e-01, -3.1279e-01, -1.2782e+00,  1.3736e+00,  7.3420e-01],\n",
       "        [ 8.5731e-01,  2.0318e-01, -8.9173e-01,  2.7562e-01,  1.6300e+00,\n",
       "         -1.6880e+00,  9.4619e-02, -1.9339e+00,  1.2229e+00,  3.7426e-01,\n",
       "          7.2901e-01, -1.5532e+00, -9.3540e-01,  6.0106e-01, -9.7636e-01,\n",
       "         -3.0427e-02,  4.7063e-01, -2.2325e+00,  1.1237e+00,  1.6323e-01],\n",
       "        [-8.3142e-03,  7.2902e-01, -2.9597e-01,  1.9768e+00,  1.2688e+00,\n",
       "          2.2298e+00, -1.7029e+00, -1.0303e-01, -2.3888e+00,  6.2677e-01,\n",
       "         -1.0536e-01,  4.9206e-01, -7.5947e-01, -2.8888e-01, -3.4009e-01,\n",
       "          1.3884e-01,  1.0273e+00,  9.0491e-01, -3.9316e-01,  3.6059e-02],\n",
       "        [ 5.5647e-01, -4.3171e-01, -8.3171e-01, -4.5925e-01,  1.4755e+00,\n",
       "         -2.2759e+00, -1.0235e+00, -1.8046e+00,  1.1024e+00, -5.4614e-01,\n",
       "         -2.2432e+00, -4.8039e-01,  6.5728e-03,  1.0200e+00,  1.2768e+00,\n",
       "         -6.1581e-01, -2.9701e-01, -1.1880e-01, -2.3218e+00, -1.1548e+00],\n",
       "        [ 3.8627e-01, -1.1113e+00,  8.2968e-01, -3.8999e-01,  8.1801e-01,\n",
       "          4.6137e-02, -4.5203e-01,  3.9605e-01,  1.5974e+00,  1.9098e+00,\n",
       "          1.4310e+00, -9.8196e-01,  7.0961e-01,  7.3463e-02, -2.1327e-01,\n",
       "         -2.9125e-01, -1.2019e+00,  1.4399e-02,  5.8802e-01,  1.2661e-01],\n",
       "        [-5.5028e-01, -8.3010e-01,  1.1588e+00, -1.4283e+00,  1.5840e+00,\n",
       "          4.4202e-01, -1.7760e+00, -2.8989e+00, -2.9264e-01,  6.4906e-01,\n",
       "          3.8303e-01, -1.0124e+00, -6.2997e-01, -1.2514e+00, -3.5687e-02,\n",
       "         -9.5439e-01, -1.5778e+00, -6.2745e-03, -1.4447e+00, -6.9120e-01],\n",
       "        [-1.5957e+00, -4.2512e-01, -1.8286e+00, -3.4996e-01, -2.6135e-01,\n",
       "          4.5120e-01, -3.2688e-01, -2.6836e-01, -1.9112e+00,  2.0332e-01,\n",
       "          1.2903e-02,  1.0327e-01,  1.2100e-01,  1.2108e+00, -2.1270e+00,\n",
       "         -1.1864e+00,  1.1581e+00,  8.3981e-01,  1.4534e+00, -4.2751e-01]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "这里只是说明一下，而后我们将补全之前的 CBOW 类代码，遍历 data 中的每个元素进行embedding。\n",
    "\n",
    "### 补全CBOW类\n",
    "\n",
    "事实上，我们对目标词和其上下文的 embedding 是在 CBOW 类中完成的。我们基于之前的示例代码进行修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        \"\"\"\n",
    "        层搭建\n",
    "        \n",
    "        输入：\n",
    "        vocab_size：去重后的单词量，本例为49个\n",
    "        embedding_dim：需要 embedding 的维度，本例设为20维\n",
    "        context_size：上下文的大小，注意和前个例子不一样，前面是上文，这里是上下文\n",
    "        \"\"\"\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * 2 * embedding_dim, 128)  # 这里乘以2应该是上下文的原因\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        输入：\n",
    "        inputs：\n",
    "        \"\"\"\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)  # 输出概率\n",
    "        return log_probs  # 结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置损失函数\n",
    "\n",
    "同前个例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = CBOW(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[228.1266691684723, 226.76646089553833, 225.41565012931824, 224.0730266571045, 222.73823070526123, 221.41210842132568, 220.09397792816162, 218.78319811820984, 217.47804927825928, 216.17788124084473, 214.88398122787476, 213.59356570243835, 212.30941009521484, 211.028706073761, 209.75223588943481, 208.47870922088623, 207.20958256721497, 205.94271397590637, 204.6795690059662, 203.41753602027893, 202.15840220451355, 200.90097188949585, 199.6455478668213, 198.3940029144287, 197.14317846298218, 195.89293932914734, 194.6439757347107, 193.394837141037, 192.14903092384338, 190.90403127670288, 189.6587804555893, 188.41311919689178, 187.16708159446716, 185.9181423187256, 184.67125380039215, 183.4199755191803, 182.16915893554688, 180.91770470142365, 179.66470193862915, 178.41098296642303, 177.15614020824432, 175.9006907939911, 174.6447789669037, 173.39031636714935, 172.13246071338654, 170.87402880191803, 169.61321783065796, 168.35220444202423, 167.09121692180634, 165.82751560211182, 164.56272161006927, 163.29687976837158, 162.0285186767578, 160.757222533226, 159.48532223701477, 158.21318328380585, 156.93696361780167, 155.65915369987488, 154.37838542461395, 153.09829366207123, 151.81558322906494, 150.53073436021805, 149.24289071559906, 147.95577758550644, 146.66496241092682, 145.37230759859085, 144.078120470047, 142.780326962471, 141.48251456022263, 140.18252730369568, 138.88134920597076, 137.57682543992996, 136.27415671944618, 134.96518343687057, 133.65768918395042, 132.34756815433502, 131.03957933187485, 129.7291655242443, 128.41824251413345, 127.10612669587135, 125.79653686285019, 124.48548477888107, 123.17237737774849, 121.8626857995987, 120.55222523212433, 119.24154990911484, 117.93361705541611, 116.62113747000694, 115.31571036577225, 114.0093582868576, 112.70405021309853, 111.40136155486107, 110.1012342274189, 108.80219835042953, 107.50758066773415, 106.21477338671684, 104.92450919747353, 103.6372562944889, 102.35313895344734, 101.0705252289772]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    for context, target in data:\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        # 这一步把上下文单词转换为字典中对应单词的值，作为tensor\n",
    "        \n",
    "        context_idxs = make_context_vector(context=context, word_to_ix=word_to_ix)\n",
    "        # context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        # 梯度归零\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        # 输出概率\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        # 这里target其实是一个数字标签，通过数字标签在字典中倒推回单词。\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
