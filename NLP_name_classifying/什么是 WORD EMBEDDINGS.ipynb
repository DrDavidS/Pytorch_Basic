{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORD EMBEDDING\n",
    "\n",
    "## 参考资料\n",
    "\n",
    "[什么是 word embedding?](https://www.zhihu.com/question/32275069/answer/61059440)\n",
    "\n",
    "[WORD EMBEDDINGS: ENCODING LEXICAL SEMANTICS](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#sphx-glr-beginner-nlp-word-embeddings-tutorial-py)\n",
    "\n",
    "[YJango的Word Embedding--介绍](https://zhuanlan.zhihu.com/p/27830489)\n",
    "\n",
    "## 概念解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码解析\n",
    "\n",
    "与我们制作独热向量（one-hot vectors）时候为每个单词定义一个唯一索引类似，在使用词嵌入的时候我们也需要为每个单词定义索引，方便查找。\n",
    "\n",
    "词嵌入被储存为了一个 $ |V|×D $ 的矩阵，其中 $ D $ 是嵌入的维度，所以某单词的索引为 $ i $，那么它就被嵌入在矩阵的第 $ i $ 行中。在代码中，从单词到索引的映射是一个名叫 word_to_ix 的字典。\n",
    "\n",
    "在 PyTorch 中，torch.nn.Embedding 可以完成词嵌入功能，包含两个参数：词表大小和嵌入维度。\n",
    "\n",
    "使用 torch.LongTensor 进行索引（这是一个 64-bit integer，带符号数）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1ee0cbea430>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Author: Robert Guthrie\n",
    "# 补充：Dr_David_S\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)  # 设定随机种子，返回一个torch.Generator对象"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们设置一个字典，用最简单的 hello world 做例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 0, 'world': 1}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "word_to_ix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 torch.nn.Embedding 方法准备进行词嵌入，参数的意义是：有2个词语，5个维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(2, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds = nn.Embedding(num_embeddings=2, embedding_dim=5)  # 2 words in vocab, 5 dimensional embeddings\n",
    "embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在将 hello 对应的值 0 转换为一个 longtensor 类型,两种方法都行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
    "lookup_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_tensor_test = torch.LongTensor([word_to_ix[\"hello\"]])\n",
    "lookup_tensor_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在使用之前定义的 embeds 模型对 hello 这个次进行词嵌入（或者说转换为词向量）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "hello_embed = embeds(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原来的代码到此就结束了，但是这里我们希望再试试对 world 的词嵌入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_tensor2 = torch.tensor([word_to_ix[\"world\"]], dtype=torch.long)\n",
    "lookup_tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1661, -1.5228,  0.3817, -1.0276, -0.5631]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "world_embed = embeds(lookup_tensor2)\n",
    "print(world_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上述结果来看，似乎对于 hello 的 embedding 没有涉及到 world？\n",
    "\n",
    "事实上，embeds只是一个随机生成的矩阵，目前其中的向量并不能代表任何意义，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519],\n",
       "        [-0.1661, -1.5228,  0.3817, -1.0276, -0.5631]], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一个 N-Gram 语言模型\n",
    "\n",
    "之前我们的 embeds 矩阵的初始值是随机生成的，现在我们将要计算一些训练样例的损失函数，然后使用反向传播去更新参数。\n",
    "\n",
    "### 文本处理\n",
    "\n",
    "首先设定参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2  # 指上下文的size\n",
    "EMBEDDING_DIM = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，上下文是指目标单词的前 $ n $ 个单词，或者后 $ n $ 个单词，或者前后都有总共 $ n $ 个单词。\n",
    "\n",
    "在本例中，我们使用的是莎士比亚的十四行诗，使用空格分词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">SONNET 2：\n",
    ">\n",
    ">当四十个冬天围攻你的朱颜，\n",
    ">\n",
    ">在你美的园地挖下深的战壕，\n",
    ">\n",
    ">你青春的华服，那么被人艳羡，\n",
    ">\n",
    ">将成褴褛的败絮，谁也不要瞧：\n",
    ">\n",
    ">那时人若问起你的美在何处，\n",
    ">\n",
    ">哪里是你那少壮年华的宝藏，\n",
    ">\n",
    ">你说，\"在我这双深陷的眼眶里，\n",
    ">\n",
    ">是贪婪的羞耻，和无益的颂扬。\"\n",
    ">\n",
    ">你的美的用途会更值得赞美，\n",
    ">\n",
    ">如果你能够说，\"我这宁馨小童\n",
    ">\n",
    ">将总结我的账，宽恕我的老迈，\"\n",
    ">\n",
    ">证实他的美在继承你的血统！\n",
    ">\n",
    ">这将使你在衰老的暮年更生，\n",
    ">\n",
    ">并使你垂冷的血液感到重温。\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When',\n",
       " 'forty',\n",
       " 'winters',\n",
       " 'shall',\n",
       " 'besiege',\n",
       " 'thy',\n",
       " 'brow,',\n",
       " 'And',\n",
       " 'dig',\n",
       " 'deep',\n",
       " 'trenches',\n",
       " 'in',\n",
       " 'thy',\n",
       " \"beauty's\",\n",
       " 'field,',\n",
       " 'Thy',\n",
       " \"youth's\",\n",
       " 'proud',\n",
       " 'livery',\n",
       " 'so',\n",
       " 'gazed',\n",
       " 'on',\n",
       " 'now,',\n",
       " 'Will',\n",
       " 'be',\n",
       " 'a',\n",
       " \"totter'd\",\n",
       " 'weed',\n",
       " 'of',\n",
       " 'small',\n",
       " 'worth',\n",
       " 'held:',\n",
       " 'Then',\n",
       " 'being',\n",
       " 'asked,',\n",
       " 'where',\n",
       " 'all',\n",
       " 'thy',\n",
       " 'beauty',\n",
       " 'lies,',\n",
       " 'Where',\n",
       " 'all',\n",
       " 'the',\n",
       " 'treasure',\n",
       " 'of',\n",
       " 'thy',\n",
       " 'lusty',\n",
       " 'days;',\n",
       " 'To',\n",
       " 'say,',\n",
       " 'within',\n",
       " 'thine',\n",
       " 'own',\n",
       " 'deep',\n",
       " 'sunken',\n",
       " 'eyes,',\n",
       " 'Were',\n",
       " 'an',\n",
       " 'all-eating',\n",
       " 'shame,',\n",
       " 'and',\n",
       " 'thriftless',\n",
       " 'praise.',\n",
       " 'How',\n",
       " 'much',\n",
       " 'more',\n",
       " 'praise',\n",
       " \"deserv'd\",\n",
       " 'thy',\n",
       " \"beauty's\",\n",
       " 'use,',\n",
       " 'If',\n",
       " 'thou',\n",
       " 'couldst',\n",
       " 'answer',\n",
       " \"'This\",\n",
       " 'fair',\n",
       " 'child',\n",
       " 'of',\n",
       " 'mine',\n",
       " 'Shall',\n",
       " 'sum',\n",
       " 'my',\n",
       " 'count,',\n",
       " 'and',\n",
       " 'make',\n",
       " 'my',\n",
       " 'old',\n",
       " \"excuse,'\",\n",
       " 'Proving',\n",
       " 'his',\n",
       " 'beauty',\n",
       " 'by',\n",
       " 'succession',\n",
       " 'thine!',\n",
       " 'This',\n",
       " 'were',\n",
       " 'to',\n",
       " 'be',\n",
       " 'new',\n",
       " 'made',\n",
       " 'when',\n",
       " 'thou',\n",
       " 'art',\n",
       " 'old,',\n",
       " 'And',\n",
       " 'see',\n",
       " 'thy',\n",
       " 'blood',\n",
       " 'warm',\n",
       " 'when',\n",
       " 'thou',\n",
       " \"feel'st\",\n",
       " 'it',\n",
       " 'cold.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按道理，我们在训练之前输入的数据应当是有标记的，我们暂时忽略这个事情（后面会说明）。\n",
    "\n",
    "然后我们要建立一个元组（tuple），元组的格式为：\n",
    "\n",
    "$$ ([word_{i-2}, word_{i-1}], word_{i}) $$\n",
    "\n",
    "其中 $word_{i}$ 就是目标词 $ target word $ \n",
    "\n",
    "根据以上方法遍历整首诗，然后生成多个元组，放入列表 trigrams 中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['When', 'forty'], 'winters'), (['forty', 'winters'], 'shall'), (['winters', 'shall'], 'besiege')]\n"
     ]
    }
   ],
   "source": [
    "# we should tokenize the input, but we will ignore that for now\n",
    "# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
    "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
    "            for i in range(len(test_sentence) - 2)]\n",
    "# print the first 3, just so you can see what they look like\n",
    "print(trigrams[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set() 函数创建一个无序不重复元素集，可以剔除掉重复的元素，同时可能可以在部分模型如 RNN 中打乱文档上下文，忽略掉 RNN 权重偏向后输入的词语的缺点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建网络\n",
    "\n",
    "接下来我们构建一个网络，名叫 NGramLanguageModeler："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上述网络来看，init 中存在三个层，分别是：\n",
    "\n",
    "- 词嵌入层（embedding层）\n",
    "- 第一个全连接层（linear1）\n",
    "- 第二个全连接层（linear2）\n",
    "\n",
    "算上前向传播的过程就是：\n",
    "\n",
    "- 词嵌入层\n",
    "- 转换为一维\n",
    "- 第一个全连接层\n",
    "- ReLU激活\n",
    "- 第二个全连接层\n",
    "- log_softmax输出\n",
    "\n",
    "### 设置损失函数和优化函数\n",
    "\n",
    "- 使用NLLLoss()，这个函数其实就是一个不带 log_softmax 的交叉熵损失函数。\n",
    "\n",
    "- 使用optim.SGD()，最传统的随机梯度下降，没什么好说的。\n",
    "\n",
    "需要注意的是，model 的输入参数有三个，第一个就是去重后的上下文三词元组数量，第二三个则是我们设定好的 CONTEXT_SIZE 和 EMBEDDING_DIM。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型\n",
    "\n",
    "注意：\n",
    "```Python\n",
    "for context, target in trigrams\n",
    "```\n",
    "这句话指的是我们的训练样本是上下文（其实只有上文），标签是当前词。这也是为什么之前没有标记的原因。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[521.0980648994446, 518.3730139732361, 515.6657829284668, 512.9775567054749, 510.3095016479492, 507.6589107513428, 505.0247368812561, 502.40775179862976, 499.8054358959198, 497.2182126045227, 494.64570355415344, 492.0870933532715, 489.54102635383606, 487.00549697875977, 484.47964549064636, 481.962007522583, 479.45440554618835, 476.95621156692505, 474.46732568740845, 471.9869587421417, 469.5133340358734, 467.04665446281433, 464.588490486145, 462.1365964412689, 459.6906900405884, 457.25116419792175, 454.8176681995392, 452.3915226459503, 449.9703652858734, 447.5540897846222, 445.1441671848297, 442.7363283634186, 440.333039522171, 437.93455052375793, 435.5385262966156, 433.1439492702484, 430.75138878822327, 428.3579592704773, 425.9627494812012, 423.56593012809753, 421.16786646842957, 418.7679147720337, 416.3632891178131, 413.9549345970154, 411.538724899292, 409.11780762672424, 406.6911025047302, 404.26082825660706, 401.8231449127197, 399.3772203922272, 396.9234616756439, 394.4617545604706, 391.9920697212219, 389.51219415664673, 387.02166223526, 384.5189263820648, 382.00625133514404, 379.4836006164551, 376.95001888275146, 374.4040505886078, 371.8484001159668, 369.2826039791107, 366.706237077713, 364.1191017627716, 361.51986932754517, 358.9105794429779, 356.2865800857544, 353.65144419670105, 351.0037078857422, 348.3459873199463, 345.6772003173828, 342.99983620643616, 340.309041261673, 337.61083817481995, 334.90166544914246, 332.18268966674805, 329.45553064346313, 326.7179515361786, 323.9707179069519, 321.21625304222107, 318.4508934020996, 315.67900490760803, 312.9022924900055, 310.1190357208252, 307.3287479877472, 304.5321259498596, 301.72700929641724, 298.9189329147339, 296.1044912338257, 293.2818977832794, 290.4567859172821, 287.62820529937744, 284.79464197158813, 281.9580993652344, 279.11934089660645, 276.27871227264404, 273.43749260902405, 270.5928192138672, 267.75193548202515, 264.9085147380829]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    for context, target in trigrams:\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们得到了一个损失逐渐降低的模型，可以看出这个模型的结果是收敛的，如果继续训练下去，可能会更准确，但是到此为止吧。\n",
    "\n",
    "## 练习：计算词嵌入：CBOW模型\n",
    "\n",
    "连续词袋模型（Continuous Bag-of-Words model ），又被称为CBOW模型，经常用于自然语言处理深度学习。\n",
    "\n",
    "CBOW模型使用一段文本的中间词作为目标词，即给出目标词的上下文单词作为训练数据。CBOW去除了上下文各词语的词序信息，使用的是上下文各词的词向量的平均值。\n",
    "\n",
    "CBOW对于一段长度为 $ n $ 的训练样本：$ w_{i-(n-1)}, ...,w_i $ ，其输入为: $$ x = \\frac {1}{n - 1}\\sum_{w_j \\in c}e(w_j) $$\n",
    "\n",
    "其中 $ w $ 是目标词，即 $ w_i $， $ c $ 是上下文，上式就是将输入求平均。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
