{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSLATION WITH A SEQUENCE TO SEQUENCE NETWORK AND ATTENTION\n",
    "\n",
    "## 参考链接\n",
    "\n",
    "[NLP FROM SCRATCH: TRANSLATION WITH A SEQUENCE TO SEQUENCE NETWORK AND ATTENTION](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n",
    "\n",
    "## 简述\n",
    "\n",
    "本教程主要会训练一个将法语翻译为英语的神经网络。\n",
    "\n",
    "```\n",
    "[KEY: > input, = target, < output]\n",
    "\n",
    "> il est en train de peindre un tableau .\n",
    "= he is painting a picture .\n",
    "< he is painting a picture .\n",
    "\n",
    "> pourquoi ne pas essayer ce vin delicieux ?\n",
    "= why not try that delicious wine ?\n",
    "< why not try that delicious wine ?\n",
    "\n",
    "> elle n est pas poete mais romanciere .\n",
    "= she is not a poet but a novelist .\n",
    "< she not not a poet but a novelist .\n",
    "\n",
    "> vous etes trop maigre .\n",
    "= you re too skinny .\n",
    "< you re all alone .\n",
    "```\n",
    "\n",
    "可以，看出取得了一定程度的成功。\n",
    "\n",
    "seq2seq 网络使用了两个 RNN 协同工作，使得一段序列变成另一段序列。编码器网络将输入序列压缩为一个向量，而解码器将这个向量展开为一个新序列。\n",
    "\n",
    "![img](https://pytorch.org/tutorials/_images/seq2seq.png)\n",
    "\n",
    "为了改进这个模型，我们将使用注意力机制（attention mechanism），它可以让解码器学会如何把注意力集中在输入序列的特定范围中。\n",
    "\n",
    "## 数据准备\n",
    "\n",
    "### 导入必要包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # 训练设备选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据文件\n",
    "\n",
    "实验所用的数据是成千上万的从英语到法语的翻译用例。\n",
    "\n",
    "> 从 [这里](https://download.pytorch.org/tutorial/data.zip) 下载数据和解压到相关的路径。\n",
    "\n",
    "该文件是是一个使用制表符（Table）分隔的翻译列表：\n",
    "\n",
    "> I am cold.    J'ai froid.\n",
    "\n",
    "与character-level RNN教程中使用的字符编码类似，我们将用语言中的每个单词作为独热向量，换句话说就是一个除了单词索引处是1，其他地方都是0的巨型向量。\n",
    "\n",
    "相对而言，一种语言中可能只有数十个字母（characters），但是单词（words）的数量可是多得多。为此，我们会小小地做一些欺骗性的数据修剪,保证每种语言只使用几千单词。\n",
    "\n",
    "![img](https://pytorch.org/tutorials/_images/word-encoding.png)\n",
    "\n",
    "我们需要给每个单词一个唯一索引，以便之后作为网络的输入（inputs）和目标（targets）。为了持续跟踪这些信息，我们会使用一个名叫 `Lang` 的帮助类，其中有 单词 → 索引 (`word2index`) 和 索引 → 单词(`index2word`) 的字典，以及每个单词的计数 `word2count` ，用于以后替换稀有单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0  # ？\n",
    "EOS_token = 1  # ？\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    \"\"\"Lang类用于辅助工作\"\"\"\n",
    "    def __init__(self, name):\n",
    "        \"\"\"\"初始化\"\"\"\n",
    "        self.name = name\n",
    "        self.word2index = {}  # 新建单词 → 索引字典\n",
    "        self.word2count = {}  # 新建单词技术字典\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}  # 新建索引 → 单词字典\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        \"\"\"\n",
    "        处理句子\n",
    "        \n",
    "        用空格来拆分句子中的单词\n",
    "        \"\"\"\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        \"\"\"\n",
    "        处理单词\n",
    "        \n",
    "        如果遇到新单词，则计入字典\n",
    "        如果是旧单词，则计数加1\n",
    "        \"\"\"\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words  # 加入单词 → 索引字典\n",
    "            self.word2count[word] = 1  # 第一次出现\n",
    "            self.index2word[self.n_words] = word  # 加入索引 → 单词字典\n",
    "            self.n_words += 1  # 单词总数加1\n",
    "        else:\n",
    "            self.word2count[word] += 1  # 统计旧单词出现次数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些文件全部采用 Unicode 编码，为了方便起见，我们将 Unicode 字母：\n",
    "- 转换成ASCII编码\n",
    "- 所有内容小写\n",
    "- 缩减标点符号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    \"\"\"将Unicode字符串转化为ASCII\"\"\"\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())  # 小写，去除前后空格\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)  # 标点符号前面增加了一个空格\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)  # 中括号定义匹配范围，+ 匹配多次,^ 表示非这些字符的，都变成空格\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面一段代码主要是将字符串从 Unicode 编码转换为 ASCII 编码。\n",
    "\n",
    "参考链接1：[What is the best way to remove accents in a Python unicode string?](https://stackoverflow.com/questions/517923/what-is-the-best-way-to-remove-accents-in-a-python-unicode-string/518232#518232)\n",
    "\n",
    "参考链接2：[python unicodedata用法](https://blog.csdn.net/xc_zhou/article/details/82079753)\n",
    "\n",
    "参考链接3：[Python3基础:String模块ascii_letters和digits](https://blog.csdn.net/killmice/article/details/53118884)\n",
    "\n",
    "具体解释可以看上一篇教程。\n",
    "\n",
    "\n",
    "总而言之，上半段代码就是先把 Unicode 编码转换为普通格式字符，同时保证这些字符是英文字符，例如：\n",
    "\n",
    ">转换前：Klüft skräms inför på fédéral électoral große\n",
    ">\n",
    ">转换后：Kluft skrams infor pa federal electoral groe\n",
    "\n",
    "下半段代码则是把字符串前后空格去掉，然后在标点符号 `!?.` 的前面加一个空格，最后如果不是大小写字母或者普通标点的则去掉。\n",
    "\n",
    "--------------\n",
    "\n",
    "我们将按行分开并将每一行分成两列（一个语句对）来读取文件。这些文件都是英语 → 其他语言，所以如果我们想从其他语言翻译 → 英语，添加reverse标志来翻转词语对。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    \"\"\"读取语言文本并处理\n",
    "    \n",
    "    lang1:str\n",
    "    lang2:str\n",
    "    \"\"\"\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize，以制表符分隔\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances，如果是反向翻译\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例句非常多，由于我们想快速训练一些东西，故我们将数据集修剪为仅包含相对较短和简单的句子。\n",
    "\n",
    "这些句子的最大长度是10个单词（包括标点符号），同时我们将那些翻译为“I am”或“he is”等形式的句子进行了修改（考虑到之前清除的标点符号——撇号 `'`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10  # 最大长度10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    \"\"\"\n",
    "    过滤器\n",
    "    \n",
    "    返回翻译语句对中带撇号开头的语句对，长度小于10\n",
    "    \"\"\"\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完整的数据准备过程：\n",
    "\n",
    "- 按行读取文本文件，将行拆分成对；\n",
    "- 规范文本，按长度和内容过滤；\n",
    "- 从句子中成对列出单词列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10599 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4345\n",
      "eng 2803\n",
      "['je suis satisfait de sa performance .', 'i m pleased with his performance .']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    \"\"\"准备数据\n",
    "    \n",
    "    lang1：str,语言1，法语\n",
    "    lang2：str,语言2，英语\n",
    "    reverse：语句对顺序是否反转，Fasle表示从法语翻译到英语\n",
    "    \"\"\"\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)  # 准备输入输出文本和语句对\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)  # 筛选符合要求的语句对\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))  # 打印剪枝后的数量\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:  # 分别保存\n",
    "        input_lang.addSentence(pair[0])  # 输入\n",
    "        output_lang.addSentence(pair[1])  # 输出\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)  # 语言名称，单词数量\n",
    "    print(output_lang.name, output_lang.n_words)  # 语言名称，单词数量\n",
    "    return input_lang, output_lang, pairs  # 输出\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)  # 反向准备\n",
    "print(random.choice(pairs))  # 随机打印一个语句对"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq模型\n",
    "\n",
    "递归神经网络（RNN）是一种对序列进行计算并利用自己的输出作为后续步骤输入的网络\n",
    "\n",
    "序列到序列网络（Sequence to Sequence network）, 也叫做 seq2seq 网络, 又或者是编码器解码器网络（Encoder Decoder network）, 是一种由两个称为编码器和解码器的RNN组成的模型。编码器读取输入序列并输出一个向量，解码器读取该向量并产生输出序列。\n",
    "\n",
    "![img](https://pytorch.org/tutorials/_images/seq2seq.png)\n",
    "\n",
    "与单个RNN进行序列预测不同，seq2seq模型将我们从序列长度和顺序中解放出来，这使得它更适合两种语言的翻译。\n",
    "\n",
    "以这句话“Je ne suis pas le chat noir” → “I am not the black cat”为例。输入句子中的大多数单词在输出句子中都可以直接被翻译，但顺序略有不同，例如: “chat noir” 和 “black cat”。由于采用 “ne/pas”结构, 故输入的句子中还有另外一个单词。因此直接从输入词的序列中直接生成正确的翻译是很困难的。\n",
    "\n",
    "使用seq2seq模型时，编码器会创建一个向量，在理想的情况下，会将输入序列的实际语义编码为单个向量 —— 相当于在 N 维语料集（句子集）空间中的一个点。\n",
    "\n",
    "### 编码器\n",
    "\n",
    "seq2seq网络的编码器是RNN，它为输入句子中的每个单词输出一些值。对于每个输入单词，编码器输出一个向量和一个隐藏状态（hidden state），并将该隐藏状态用于下一个输入的单词。\n",
    "\n",
    "![img](https://pytorch.org/tutorials/_images/encoder-network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"编写一个编码器RNN\"\"\"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size  # 隐藏状态大小\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)  \n",
    "        # input_size\n",
    "        # hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)  # 门控循环单元（GRU）\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)  # embedding输出，整理维度\n",
    "        output = embedded \n",
    "        output, hidden = self.gru(output, hidden)  # 过一次门控循环单元\n",
    "        return output, hidden  # 输出结果和隐藏状态，准备进入下一个时间步\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)  # 初始化隐藏状态，全0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解码器\n",
    "\n",
    "解码器是另一个RNN，它接收编码器输出的向量，然后输出一个单词序列来完成翻译。\n",
    "\n",
    "#### 简单的解码器\n",
    "\n",
    "在最简单的seq2seq解码器中，我们只使用编码器的最后输出。这个最后输出有时也被称为*上下文向量（context vector）*因为它从整个序列中编码上下文。该上下文向量用作解码器的初始隐藏状态。\n",
    "\n",
    "在解码的每一步,解码器都被赋予一个输入指令和隐藏状态。初始输入指令是字符串开始的`<SOS>`指令,而第一个隐藏状态是上下文向量(编码器的最后隐藏状态).\n",
    "    \n",
    "![img](https://pytorch.apachecn.org/docs/1.0/img/34b376e0c7299810f7349ab99c2c5497.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\"编写一个解码器RNN\"\"\"\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        \"\"\"初始化，定义相关层\"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size  # 隐藏状态的大小\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)  # 定义embedding\n",
    "        # output_size：词的数量\n",
    "        # hidden_size：embedding的维度\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)  # 定义门控循环单元\n",
    "        self.out = nn.Linear(hidden_size, output_size)  # 定义线性层\n",
    "        self.softmax = nn.LogSoftmax(dim=1)  # softmax\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        output = self.embedding(input).view(1, 1, -1)  # 对输入embedding\n",
    "        output = F.relu(output)  # 激活函数\n",
    "        output, hidden = self.gru(output, hidden)  # 经过GRU输出，同时记录隐藏状态\n",
    "        output = self.softmax(self.out(output[0]))  # 经过softmax输出概率\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)  # 初始化隐藏层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们鼓励你训练并观察这个模型的结果，但为了节省空间，我们将直入主题开始讲解注意力机制。\n",
    "\n",
    "### 带注意力机制的解码器\n",
    "\n",
    "如果在编码器和解码器之间仅传递上下文向量，则该单个向量承担编码整个句子的重任。\n",
    "\n",
    "注意力机制允许解码器网络针对解码器自身输出的每一步，“聚焦”于编码器输出的某个不同部分。首先我们计算一组*注意力权重*。将它们与编码器输出向量相乘以创建加权组合。该结果(在代码中称为 `attn_applied`) 应该包含关于输入序列的特定部分的信息，从而帮助解码器选择正确的输出单词。\n",
    "\n",
    "![img](https://pytorch.org/tutorials/_images/attention-decoder-network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">注意：\n",
    ">\n",
    ">还有其他形式的注意力通过使用相对位置方法来解决长度限制. 阅读关于 “local attention” 在 [基于注意力机制的神经机器翻译的有效途径](https://arxiv.org/abs/1508.04025)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
