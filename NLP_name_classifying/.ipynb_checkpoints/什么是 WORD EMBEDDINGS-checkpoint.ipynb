{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORD EMBEDDING\n",
    "\n",
    "## 参考资料\n",
    "\n",
    "[什么是 word embedding?](https://www.zhihu.com/question/32275069/answer/61059440)\n",
    "\n",
    "[WORD EMBEDDINGS: ENCODING LEXICAL SEMANTICS](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#sphx-glr-beginner-nlp-word-embeddings-tutorial-py)\n",
    "\n",
    "[YJango的Word Embedding--介绍](https://zhuanlan.zhihu.com/p/27830489)\n",
    "\n",
    "## 概念解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码解析\n",
    "\n",
    "与我们制作独热向量（one-hot vectors）时候为每个单词定义一个唯一索引类似，在使用词嵌入的时候我们也需要为每个单词定义索引，方便查找。\n",
    "\n",
    "词嵌入被储存为了一个 $ |V|×D $ 的矩阵，其中 $ D $ 是嵌入的维度，所以某单词的索引为 $ i $，那么它就被嵌入在矩阵的第 $ i $ 行中。在代码中，从单词到索引的映射是一个名叫 word_to_ix 的字典。\n",
    "\n",
    "在 PyTorch 中，torch.nn.Embedding 可以完成词嵌入功能，包含两个参数：词表大小和嵌入维度。\n",
    "\n",
    "使用 torch.LongTensor 进行索引（这是一个 64-bit integer，带符号数）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x23300276650>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Author: Robert Guthrie\n",
    "# 补充：Dr_David_S\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)  # 设定随机种子，返回一个torch.Generator对象"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们设置一个字典，用最简单的 hello world 做例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 0, 'world': 1}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "word_to_ix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 torch.nn.Embedding 方法准备进行词嵌入，参数的意义是：有2个词语，5个维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(2, 5)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds = nn.Embedding(num_embeddings=2, embedding_dim=5)  # 2 words in vocab, 5 dimensional embeddings\n",
    "embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在将 hello 对应的值 0 转换为一个 longtensor 类型,两种方法都行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
    "lookup_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_tensor_test = torch.LongTensor([word_to_ix[\"hello\"]])\n",
    "lookup_tensor_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在使用之前定义的 embeds 模型对 hello 这个次进行词嵌入（或者说转换为词向量）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3968, -0.6571, -1.6428,  0.9803, -0.0421]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "hello_embed = embeds(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原来的代码到此就结束了，但是这里我们希望再试试对 world 的词嵌入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_tensor2 = torch.tensor([word_to_ix[\"world\"]], dtype=torch.long)\n",
    "lookup_tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8206,  0.3133, -1.1352,  0.3773, -0.2824]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "world_embed = embeds(lookup_tensor2)\n",
    "print(world_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上述结果来看，似乎对于 hello 的 embedding 没有涉及到 world？\n",
    "\n",
    "事实上，embeds只是一个随机生成的矩阵，目前其中的向量并不能代表任何意义，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.3968, -0.6571, -1.6428,  0.9803, -0.0421],\n",
       "        [-0.8206,  0.3133, -1.1352,  0.3773, -0.2824]], requires_grad=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一个 N-Gram 语言模型\n",
    "\n",
    "之前我们的 embeds 矩阵的初始值是随机生成的，现在我们将要计算一些训练样例的损失函数，然后使用反向传播去更新参数。\n",
    "\n",
    "### 文本处理\n",
    "\n",
    "首先设定参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2  # 指上下文的size\n",
    "EMBEDDING_DIM = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本例中，我们使用的是莎士比亚的十四行诗，使用空格分词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When',\n",
       " 'forty',\n",
       " 'winters',\n",
       " 'shall',\n",
       " 'besiege',\n",
       " 'thy',\n",
       " 'brow,',\n",
       " 'And',\n",
       " 'dig',\n",
       " 'deep',\n",
       " 'trenches',\n",
       " 'in',\n",
       " 'thy',\n",
       " \"beauty's\",\n",
       " 'field,',\n",
       " 'Thy',\n",
       " \"youth's\",\n",
       " 'proud',\n",
       " 'livery',\n",
       " 'so',\n",
       " 'gazed',\n",
       " 'on',\n",
       " 'now,',\n",
       " 'Will',\n",
       " 'be',\n",
       " 'a',\n",
       " \"totter'd\",\n",
       " 'weed',\n",
       " 'of',\n",
       " 'small',\n",
       " 'worth',\n",
       " 'held:',\n",
       " 'Then',\n",
       " 'being',\n",
       " 'asked,',\n",
       " 'where',\n",
       " 'all',\n",
       " 'thy',\n",
       " 'beauty',\n",
       " 'lies,',\n",
       " 'Where',\n",
       " 'all',\n",
       " 'the',\n",
       " 'treasure',\n",
       " 'of',\n",
       " 'thy',\n",
       " 'lusty',\n",
       " 'days;',\n",
       " 'To',\n",
       " 'say,',\n",
       " 'within',\n",
       " 'thine',\n",
       " 'own',\n",
       " 'deep',\n",
       " 'sunken',\n",
       " 'eyes,',\n",
       " 'Were',\n",
       " 'an',\n",
       " 'all-eating',\n",
       " 'shame,',\n",
       " 'and',\n",
       " 'thriftless',\n",
       " 'praise.',\n",
       " 'How',\n",
       " 'much',\n",
       " 'more',\n",
       " 'praise',\n",
       " \"deserv'd\",\n",
       " 'thy',\n",
       " \"beauty's\",\n",
       " 'use,',\n",
       " 'If',\n",
       " 'thou',\n",
       " 'couldst',\n",
       " 'answer',\n",
       " \"'This\",\n",
       " 'fair',\n",
       " 'child',\n",
       " 'of',\n",
       " 'mine',\n",
       " 'Shall',\n",
       " 'sum',\n",
       " 'my',\n",
       " 'count,',\n",
       " 'and',\n",
       " 'make',\n",
       " 'my',\n",
       " 'old',\n",
       " \"excuse,'\",\n",
       " 'Proving',\n",
       " 'his',\n",
       " 'beauty',\n",
       " 'by',\n",
       " 'succession',\n",
       " 'thine!',\n",
       " 'This',\n",
       " 'were',\n",
       " 'to',\n",
       " 'be',\n",
       " 'new',\n",
       " 'made',\n",
       " 'when',\n",
       " 'thou',\n",
       " 'art',\n",
       " 'old,',\n",
       " 'And',\n",
       " 'see',\n",
       " 'thy',\n",
       " 'blood',\n",
       " 'warm',\n",
       " 'when',\n",
       " 'thou',\n",
       " \"feel'st\",\n",
       " 'it',\n",
       " 'cold.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按道理，我们在训练之前输入的数据应当是有标记的，我们暂时忽略这个事情。\n",
    "\n",
    "然后我们要建立一个元组（tuple），元组的格式为：\n",
    "\n",
    "$ ([word_{i-2}, word_{i-1}], word_{i}) $\n",
    "\n",
    "其中 $word_{i}$ 就是目标词 $ target word $ \n",
    "\n",
    "根据以上方法遍历整首诗，然后生成多个元组，放入列表 trigrams 中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['When', 'forty'], 'winters'), (['forty', 'winters'], 'shall'), (['winters', 'shall'], 'besiege')]\n"
     ]
    }
   ],
   "source": [
    "# we should tokenize the input, but we will ignore that for now\n",
    "# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
    "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
    "            for i in range(len(test_sentence) - 2)]\n",
    "# print the first 3, just so you can see what they look like\n",
    "print(trigrams[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set() 函数创建一个无序不重复元素集，可以剔除掉重复的元素，同时可能可以在部分模型如 RNN 中打乱文档上下文，忽略掉 RNN 权重偏向后输入的词语的缺点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建网络\n",
    "\n",
    "接下来我们构建一个网络，名叫 NGramLanguageModeler："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上述网络来看，init 中存在三个层，分别是：\n",
    "\n",
    "- 词嵌入层（embedding层）\n",
    "- 第一个全连接层（linear1）\n",
    "- 第二个全连接层（linear2）\n",
    "\n",
    "算上前向传播的过程就是：\n",
    "\n",
    "- 词嵌入层\n",
    "- 转换为一维\n",
    "- 第一个全连接层\n",
    "- ReLU激活\n",
    "- 第二个全连接层\n",
    "- log_softmax输出\n",
    "\n",
    "### 设置损失函数和优化函数\n",
    "\n",
    "- 使用NLLLoss()，这个函数其实就是一个不带 log_softmax 的交叉熵损失函数。\n",
    "\n",
    "- 使用optim.SGD()，最传统的随机梯度下降，没什么好说的。\n",
    "\n",
    "需要注意的是，model 的输入参数有三个，第一个就是去重后的上下文三词元组数量，第二三个则是我们设定好的 CONTEXT_SIZE 和 EMBEDDING_DIM。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型\n",
    "\n",
    "注意：\n",
    "```Python\n",
    "for context, target in trigrams\n",
    "```\n",
    "这句话指的是我们的训练样本是上下文（其实只有上文），标签是当前词。这也是为什么之前没有标记的原因。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[521.5358467102051, 519.3320527076721, 517.1413388252258, 514.9623324871063, 512.7935588359833, 510.63494420051575, 508.48767471313477, 506.35063123703003, 504.22352933883667, 502.10339069366455, 499.99057722091675, 497.8842759132385, 495.78450560569763, 493.6895911693573, 491.60060000419617, 489.51736998558044, 487.43918466567993, 485.36392521858215, 483.2929391860962, 481.2255289554596, 479.1592433452606, 477.09503626823425, 475.0319311618805, 472.97029304504395, 470.90980434417725, 468.84932231903076, 466.7888593673706, 464.72660088539124, 462.66323804855347, 460.59968090057373, 458.5350468158722, 456.4696877002716, 454.40329480171204, 452.3341739177704, 450.2626030445099, 448.18685150146484, 446.10712790489197, 444.020224571228, 441.93109941482544, 439.8375494480133, 437.7391769886017, 435.6344494819641, 433.52389907836914, 431.40721893310547, 429.28481245040894, 427.15508675575256, 425.0167257785797, 422.8728313446045, 420.719361782074, 418.5572040081024, 416.387389421463, 414.2053008079529, 412.01550674438477, 409.81563568115234, 407.6057884693146, 405.3858404159546, 403.1536753177643, 400.91347193717957, 398.6615035533905, 396.40234541893005, 394.13166856765747, 391.85190081596375, 389.5594131946564, 387.2570309638977, 384.9409773349762, 382.6167550086975, 380.27905464172363, 377.9318323135376, 375.57236528396606, 373.20215368270874, 370.81904196739197, 368.4263427257538, 366.0208230018616, 363.6078658103943, 361.1828384399414, 358.74661564826965, 356.2991724014282, 353.841206073761, 351.3721914291382, 348.89326524734497, 346.4037389755249, 343.90470695495605, 341.3948829174042, 338.8769690990448, 336.3486704826355, 333.8109886646271, 331.2659730911255, 328.71073150634766, 326.147784948349, 323.5752418041229, 320.99445605278015, 318.40504574775696, 315.80721640586853, 313.2028720378876, 310.59059476852417, 307.9719524383545, 305.3488690853119, 302.7177290916443, 300.0823497772217, 297.441862821579, 294.795893907547, 292.144127368927, 289.48970675468445, 286.83042669296265, 284.16874408721924, 281.50395584106445, 278.8324189186096, 276.16080260276794, 273.4875600337982, 270.81216859817505, 268.1361129283905, 265.4590473175049, 262.78265833854675, 260.10499262809753, 257.4300603866577, 254.75761651992798, 252.08473587036133, 249.41690301895142, 246.75200247764587, 244.08824157714844, 241.43107533454895, 238.77863597869873, 236.13069128990173, 233.4913957118988, 230.85789585113525, 228.2298891544342, 225.61023926734924, 222.99711918830872, 220.39344334602356, 217.80043268203735, 215.21440386772156, 212.63984489440918, 210.07726526260376, 207.522691488266, 204.98286485671997, 202.45330715179443, 199.94086599349976, 197.43822526931763, 194.95163750648499, 192.47679138183594, 190.01737761497498, 187.57620477676392, 185.15074825286865, 182.73976731300354, 180.34549808502197, 177.96959376335144, 175.60922956466675, 173.2685170173645, 170.94471836090088, 168.64106941223145]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    for context, target in trigrams:\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
