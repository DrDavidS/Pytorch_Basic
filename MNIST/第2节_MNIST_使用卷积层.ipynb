{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch与MNIST\n",
    "\n",
    "## 说明\n",
    "\n",
    "程序源代码参考：https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "\n",
    "本代码是官方示例的简化分析版本，网络结构和官方版本一致。对于MNSIT的基本分析参考第一节。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST数据集\n",
    "\n",
    "MNIST的图像数据是28像素×28像素的灰度图像（1通道），各个像素的取值在0到255之间。每个图像都相应地标有“1”、“2”、“3”等标签\n",
    "\n",
    "## 设定网络结构\n",
    "\n",
    "- 第一个__init__是初始化的参数，包含两个卷积层(conv)和两个全连接层(fc)。\n",
    "\n",
    "```Python\n",
    "torch.nn.Linear(in_features, out_features, bias=True)\n",
    "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "> - conv1采用Conv2d层实现，其参数意义为：输入通道为1（灰度图像），输出通道为20（人工设定），卷积核为5×5，步长为1。\n",
    ">\n",
    "> - conv2也采用Conv2d层实现，其参数意义为：输入通道为20（同conv1输出），输出通道为50（人工设定），卷积核为5×5，步长为1。\n",
    ">\n",
    "> - fc1采用线性变换层实现，输入样本大小为4\\*4\\*50（50个4\\*4的核），输出样本大小为500（人工设定）\n",
    ">\n",
    ">   特别说明：\n",
    ">   1. 50就是conv2的输出通道，相当于这里输出了50个卷积核。\n",
    ">   2. 4\\*4是上一步池化后输出的核的面积。\n",
    ">\n",
    "> - fc2也采用线性变换层实现，输入样本大小为500（同fc1输出），输出样本大小为10（十个分类）\n",
    "\n",
    "- 第二个函数forward指的是前向传播。前向传播指明了传播方向，最后经过log_softmax输出结果。顺序如下：\n",
    "\n",
    "> - conv1\n",
    "> - relu\n",
    "> - max_pool2d\n",
    "> - conv2\n",
    "> - relu\n",
    "> - max_pool2d\n",
    "> - 一维数据（view函数类似于reshape，是一种矩阵变换，这里变换是因为下面的全连接层只能接受一维数据）\n",
    "> - fc1\n",
    "> - relu\n",
    "> - fc2\n",
    "> - log_softmax（输出）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=20, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=20, out_channels=50, kernel_size=5, stride=1)\n",
    "        self.fc1 = nn.Linear(in_features=4*4*50, out_features=500)\n",
    "        self.fc2 = nn.Linear(in_features=500, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对view()函数的测试\n",
    "\n",
    "使用 torch.arange() 建立一个1D tensor，然后使用view试一试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(1, 17)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12],\n",
      "        [13, 14, 15, 16]])\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12],\n",
      "        [13, 14, 15, 16]])\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12],\n",
      "        [13, 14, 15, 16]])\n",
      "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12, 13, 14, 15, 16]])\n",
      "tensor([[ 1,  2],\n",
      "        [ 3,  4],\n",
      "        [ 5,  6],\n",
      "        [ 7,  8],\n",
      "        [ 9, 10],\n",
      "        [11, 12],\n",
      "        [13, 14],\n",
      "        [15, 16]])\n"
     ]
    }
   ],
   "source": [
    "print(a.view(4, 4))\n",
    "print(a.view(4, -1))\n",
    "print(a.view(-1, 4))\n",
    "print(a.view(2, -1))\n",
    "print(a.view(-1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，view(row, col)函数会自动变换tensor的维度，-1就是自动判断行或者列数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载MNIST数据集\n",
    "\n",
    "下列加载代码分为三步：\n",
    "1. 定义数据变换规则\n",
    "2. 读取MNIST数据集\n",
    "3. 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tf = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "mnist_trainset = datasets.MNIST(root='./MNIST_data/MNIST', train=True, download=True, transform=data_tf)\n",
    "mnist_testset = datasets.MNIST(root='./MNIST_data/MNIST', train=False, download=True, transform=data_tf)\n",
    "\n",
    "train_loader = DataLoader(mnist_trainset, batch_size=1000, shuffle=True)\n",
    "test_loader = DataLoader(mnist_testset, batch_size=1000, shuffle=True)  # 测试集无需打乱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练神经网络\n",
    "\n",
    "### 加载网络\n",
    "\n",
    "网络初始化代码如下。其中：优化器为SGD，损失函数为交叉熵损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available.\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "if torch.cuda.is_available():  # 如果GPU可以使用\n",
    "    net = net.cuda(1)\n",
    "    print(\"CUDA is available.\")\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
      "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始训练网络\n",
    "\n",
    "参照第1节代码，5个epoch，每个batch有1000个数据。\n",
    "\n",
    ">注意区别：\n",
    ">1. 去掉了CUDA是否可用的判断代码\n",
    ">2. 这里由于输入后第一层为卷积层，接收的是二维图像数据，所以不用把img变换为一维数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch_ndx: 0, loss: 2.304\n",
      "epoch: 0, batch_ndx: 10, loss: 2.253\n",
      "epoch: 0, batch_ndx: 20, loss: 2.205\n",
      "epoch: 0, batch_ndx: 30, loss: 2.139\n",
      "epoch: 0, batch_ndx: 40, loss: 2.064\n",
      "epoch: 0, batch_ndx: 50, loss: 1.95\n",
      "epoch: 1, batch_ndx: 0, loss: 1.785\n",
      "epoch: 1, batch_ndx: 10, loss: 1.567\n",
      "epoch: 1, batch_ndx: 20, loss: 1.364\n",
      "epoch: 1, batch_ndx: 30, loss: 1.177\n",
      "epoch: 1, batch_ndx: 40, loss: 0.9952\n",
      "epoch: 1, batch_ndx: 50, loss: 0.8769\n",
      "epoch: 2, batch_ndx: 0, loss: 0.779\n",
      "epoch: 2, batch_ndx: 10, loss: 0.6975\n",
      "epoch: 2, batch_ndx: 20, loss: 0.6161\n",
      "epoch: 2, batch_ndx: 30, loss: 0.5728\n",
      "epoch: 2, batch_ndx: 40, loss: 0.5286\n",
      "epoch: 2, batch_ndx: 50, loss: 0.5265\n",
      "epoch: 3, batch_ndx: 0, loss: 0.5426\n"
     ]
    }
   ],
   "source": [
    "net.train()  # 启用train模式\n",
    "for epoch in range(5):\n",
    "    for batch_ndx, data in enumerate(train_loader):  # 按照一个batch = 1000来抽取数据\n",
    "        img, label = data        \n",
    "        # 前向传播\n",
    "        if torch.cuda.is_available():\n",
    "            img = img.cuda(1)\n",
    "            label = label.cuda(1)\n",
    "            # print(\"使用CUDA训练\")\n",
    "        else:\n",
    "            pass\n",
    "        output = net(img)\n",
    "        loss = loss_func(output, label)\n",
    "        \n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()  # 梯度归零\n",
    "        loss.backward()  # 损失函数反向传播\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_ndx%10 == 0:\n",
    "            print('epoch: {}, batch_ndx: {}, loss: {:.4}'.format(epoch, batch_ndx, loss.data.item()))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试网络\n",
    "\n",
    "代码同第1节。删除了对CUDA的判断及开始的数据一维化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "Test Loss: 0.311799, Acc: 0.913100\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "eval_loss = 0\n",
    "eval_acc = 0\n",
    "\n",
    "# 对测试集进行测试\n",
    "for batch_ndx, data in enumerate(test_loader):\n",
    "    # 获得img(手写图片)，label标签（手写图片对应数字）\n",
    "    img, label = data\n",
    "    if torch.cuda.is_available():\n",
    "        img = img.cuda(1)\n",
    "        label = label.cuda(1)\n",
    "        # print(\"使用CUDA训练\")\n",
    "    else:\n",
    "        pass    \n",
    "    #  向前传播，获得out结果和损失函数\n",
    "    output = net(img)\n",
    "    loss = loss_func(output, label)  # 交叉熵\n",
    "    \n",
    "    # 损失函数乘标签大小累计\n",
    "    eval_loss += loss.data.item()*label.size(0)  # 每个计算一次损失？\n",
    "    # 在10维数据中，获得最大的预测值（即预测数）\n",
    "    _, pred = torch.max(output, 1)\n",
    "    # 判断是否与真实结果相同\n",
    "    num_correct = (pred == label).sum()\n",
    "    \n",
    "    # 累计真实结果\n",
    "    eval_acc += num_correct.item()\n",
    "\n",
    "print(label.size(0))\n",
    "# 输出评估结果    \n",
    "print('Test Loss: {:.6f}, Acc: {:.6f}'.format(\n",
    "    eval_loss / (len(mnist_testset)),\n",
    "    eval_acc / (len(mnist_testset))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
